## Stanford CS 224n Natural Language Processing with Deep Learning - (Winter 2019)

Self-study on Stanford's CS 224n, Winter 2019, here you will find coursework lecture notes,
completed assignments and extra resource links.

---

### Lecture 1 - Introduction and Word Vectors 

1. Watch
   - [Introduction and Word Vectors](https://youtu.be/8rXD5-xhemo)
2. Read
   - [Word2Vec Tutorial Part 1 - The Skip-Gram Model - by Chris McCormik](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)
   - [Word2Vec Tutorial Part 2 - Negative Sampling - by Chris McCormik](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)
   - [Sebastian Ruder's Word Embeddings  (Part 1 - Part 5)](https://ruder.io/word-embeddings-1/)
3. Tutorials
   - [How to Develop Word Embeddings in Python with Gensim - by Jason Brownlee](https://machinelearningmastery.com/develop-word-embeddings-python-gensim/)
   - [Gensim Word2Vec Tutorial â€“ Full Working Example - by Kavita Ganesan](https://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/)
4. Assignment
   - Coding - [a1](assignments/a1/exploring_word_vectors.ipynb)
5. Notes
   - [word2vec1](notes/notes01.pdf)

## LICENSE
All slides, notes, assignments, and provided code scaffolds are owned by Stanford University.

## Acknowlegements
Special thanks to Stanford and Professor Chris Manning for making this great resources online and free to the public. No access to autograder, thus no guarantee that the solutions are correct.
